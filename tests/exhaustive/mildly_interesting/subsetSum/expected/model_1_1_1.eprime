language ESSENCE' 1.0

given s: int
given fin1: int
given fin2: int
given fin3: int
given nums_Explicit: matrix indexed by [int(1..fin1)] of int(fin2..fin3)
letting let1 be fin1
letting let2 be [nums_Explicit[q5] | q5 : int(1..fin1)]
find x_ExplicitVarSizeWithMarker_Marker: int(0..let1)
find x_ExplicitVarSizeWithMarker_Values: matrix indexed by [int(1..let1)] of int(let2)
such that
    and([q8 <= x_ExplicitVarSizeWithMarker_Marker ->
         or([nums_Explicit[q10] = x_ExplicitVarSizeWithMarker_Values[q8] | q10 : int(1..fin1)])
             | q8 : int(1..let1)]),
    s =
    sum([toInt(q6 <= x_ExplicitVarSizeWithMarker_Marker) * catchUndef(x_ExplicitVarSizeWithMarker_Values[q6], 0)
             | q6 : int(1..let1)]),
    and([q2 + 1 <= x_ExplicitVarSizeWithMarker_Marker ->
         x_ExplicitVarSizeWithMarker_Values[q2] < x_ExplicitVarSizeWithMarker_Values[q2 + 1]
             | q2 : int(1..let1 - 1)]),
    and([q3 > x_ExplicitVarSizeWithMarker_Marker -> x_ExplicitVarSizeWithMarker_Values[q3] = min(let2)
             | q3 : int(1..let1)]),
    1 <= x_ExplicitVarSizeWithMarker_Marker,
    x_ExplicitVarSizeWithMarker_Marker <= let1

